{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8c6e12",
   "metadata": {},
   "source": [
    "## Simple Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b1674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import yaml\n",
    "from src import utils\n",
    "import time\n",
    "from pprint import pprint\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd0fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test dataset\n",
    "\n",
    "# GSM8K 8792 (7473/1319)\n",
    "gsm8k = '/userhomes/philhoon/kt-ai-challenge/data/openai-gpt-3.5-turbo-0301-2048-0.0-KOR-gsm8k-test.jsonl'\n",
    "\n",
    "# MWP_KR_DATA 3750\n",
    "mwp_kr = '/userhomes/philhoon/kt-ai-challenge/data/public_mwp_data_v2_reasoning-test.jsonl'\n",
    "\n",
    "# AddSub 395\n",
    "addsub = '/userhomes/philhoon/kt-ai-challenge/eng_data/kor-D_addsub-test.json'\n",
    "\n",
    "# MultiArith 600\n",
    "multiarith = '/userhomes/philhoon/kt-ai-challenge/eng_data/kor-D_multiarith-test.json'\n",
    "\n",
    "# SingleEq 395\n",
    "single_eq = '/userhomes/philhoon/kt-ai-challenge/eng_data/kor-D_single_eq-test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84716cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_data = utils.read_jsonlines(gsm8k)\n",
    "mwp_kr_data = utils.read_jsonlines(mwp_kr)\n",
    "addsub_data = utils.read_jsonlines(addsub)\n",
    "multiarith_data = utils.read_jsonlines(multiarith)\n",
    "single_eq_data = utils.read_jsonlines(single_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e00b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_api():\n",
    "    def __init__(self, url, headers):\n",
    "        self.url = url\n",
    "        self.headers = headers\n",
    "        \n",
    "    def generate(self, body):\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.url, data=body, headers=self.headers, verify=False)\n",
    "        \n",
    "        except:\n",
    "            print(\"retrying due to an error......\")\n",
    "            time.sleep(20)\n",
    "            return self.generate(body)\n",
    "        \n",
    "        return response\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fcb24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_kt = '/userhomes/philhoon/kt-ai-challenge/api-peft3.json'\n",
    "api_kt = utils.read_json_file(api_kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73402f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kt = model_api(api_kt['url'], api_kt['headers'])\n",
    "BODY_TEMP = api_kt['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "784da61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = 'openai'\n",
    "api_key_file = '/userhomes/philhoon/kt-ai-challenge/api-openai.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da6c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name=\"gpt-3.5-turbo-0301\",\n",
    "model_name = \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "max_token=2048\n",
    "temperature=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c4c48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = single_eq\n",
    "data = utils.read_jsonlines(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a7c2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai key\n",
    "key_dict = utils.get_keys(api_key_file)\n",
    "openai.api_key = key_dict[api]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e421b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file = '/userhomes/philhoon/kt-ai-challenge/data/public_mwp_data_v2_reasoning.jsonl'\n",
    "# print(f'output_file : {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7283f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai-gpt-3.5-turbo-0301-2048-0.0-gsm8k-test.jsonl -> openai-gpt-3.5-turbo-0301-3p-2048-0.0-gsm8k-test.jsonl\n",
    "# openai-gpt-3.5-turbo-0301-gsm8k-test.jsonl -> openai-gpt-3.5-turbo-0301-3p-inf-1.0-gsm8k-test.jsonl\n",
    "# openai-gpt-3.5-turbo-0301-2048-0.0-gsm8k-test.jsonl -> openai-gpt-3.5-turbo-0301-3p-2048-0.0-gsm8k-test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2203391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file = output_path + '/' + 'gsm8k-korean.jsonl'\n",
    "# print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9e0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jsonlines writier\n",
    "# jsonl_writer = utils.JSONLWriter(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74b79621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = utils.read_json_file(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58465330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c04aa110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bab52c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_line_split(context):\n",
    "    context = re.sub(r'\\n{1,}', ' ', context)\n",
    "    context = re.sub(r'\\s{2,}', ' ', context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87266483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(message, model_name, max_token, temperature):\n",
    "    \"\"\"\n",
    "    Generate response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=message,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_token,\n",
    "            n=1)\n",
    "    except:\n",
    "        print(\"retrying due to an error......\")\n",
    "        time.sleep(20)\n",
    "        return generate_answer(message, model_name, max_token, temperature)\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b621c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kor_msg(context):\n",
    "    \"\"\"\n",
    "    Message for chat api\n",
    "    \"\"\"\n",
    "\n",
    "    msg = [\n",
    "        {\n",
    "            \"role\" : \"system\",\n",
    "            \"content\" : \"다음 질문에 차근차근 답하시오.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : context\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32444841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0785389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kt_verfiy(que, ans):\n",
    "    prompt = f'해당 질문에 대한 답변이 맞는지 확인하세요 질문:{que} 답변:{ans}'\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c43d8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(prompt, body_temp):\n",
    "    body_temp[\"utterance\"] = prompt\n",
    "    return json.dumps(body_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8d41271",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                | 0/102 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_msg: \n",
      "[{'role': 'system', 'content': '다음 질문에 차근차근 답하시오.'}, {'role': 'user', 'content': '헛간에는 28개의 건초 더미가 있었습니다. 팀은 오늘 헛간에 더 많은 건초 더미를 쌓았습니다. 이제 헛간에는 54개의 건초 더미가 있습니다. 그는 헛간에 몇 개의 건초 더미를 저장했을까요?'}]\n",
      "q_response: \n",
      "팀이 헛간에 쌓은 건초 더미의 개수를 알기 위해서는 헛간에 있던 건초 더미의 개수를 빼야 합니다. 헛간에 있던 건초 더미의 개수는 54개 - 28개 = 26개입니다. 따라서, 팀은 헛간에 26개의 건초 더미를 저장했습니다.\n",
      "----------------------------\n",
      "prompt : \n",
      "해당 질문에 대한 답변이 맞는지 확인하세요 질문:헛간에는 28개의 건초 더미가 있었습니다. 팀은 오늘 헛간에 더 많은 건초 더미를 쌓았습니다. 이제 헛간에는 54개의 건초 더미가 있습니다. 그는 헛간에 몇 개의 건초 더미를 저장했을까요? 답변:팀이 헛간에 쌓은 건초 더미의 개수를 알기 위해서는 헛간에 있던 건초 더미의 개수를 빼야 합니다. 헛간에 있던 건초 더미의 개수는 54개 - 28개 = 26개입니다. 따라서, 팀은 헛간에 26개의 건초 더미를 저장했습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhomes/philhoon/miniconda3/envs/self-debate/lib/python3.8/site-packages/urllib3/connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'aiapi.genielabs.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "  1%|█▎                                                                                                                                      | 1/102 [00:09<16:15,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : \n",
      "\"그는 헛간에 28개의 건초 더미를 쌓았습니다. 팀은 오늘 헛간에 더 많은 건초 더미를 쌓았습니다. 이제 헛간에는 54개의 건초 더미가 있습니다. 그는 헛간에 몇 개의 건초 더미를 저장했을까요?\"\n",
      "----------------------------\n",
      "answer : \n",
      "26.0\n",
      "============================\n",
      "query_msg: \n",
      "[{'role': 'system', 'content': '다음 질문에 차근차근 답하시오.'}, {'role': 'user', 'content': '레스토랑에서 식사를 한 후, 샐리, 샘, 앨리사는 계산서를 균등하게 나누기로 결정했습니다. 각각의 사람이 45달러를 지불했다면, 총 계산서는 얼마인가요?'}]\n",
      "q_response: \n",
      "세 사람이 총 45달러를 지불했으므로, 세 사람의 지불 금액을 합산하면 총 계산서 금액을 구할 수 있습니다. 45달러 x 3 = 135달러 따라서, 총 계산서 금액은 135달러입니다.\n",
      "----------------------------\n",
      "prompt : \n",
      "해당 질문에 대한 답변이 맞는지 확인하세요 질문:레스토랑에서 식사를 한 후, 샐리, 샘, 앨리사는 계산서를 균등하게 나누기로 결정했습니다. 각각의 사람이 45달러를 지불했다면, 총 계산서는 얼마인가요? 답변:세 사람이 총 45달러를 지불했으므로, 세 사람의 지불 금액을 합산하면 총 계산서 금액을 구할 수 있습니다. 45달러 x 3 = 135달러 따라서, 총 계산서 금액은 135달러입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhomes/philhoon/miniconda3/envs/self-debate/lib/python3.8/site-packages/urllib3/connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'aiapi.genielabs.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "  2%|██▋                                                                                                                                     | 2/102 [00:17<13:54,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : \n",
      "\"세 사람이 각각 45달러를 지불했으므로\n",
      "----------------------------\n",
      "answer : \n",
      "135.0\n",
      "============================\n",
      "query_msg: \n",
      "[{'role': 'system', 'content': '다음 질문에 차근차근 답하시오.'}, {'role': 'user', 'content': '메리는 케이크를 굽고 있습니다. 레시피에는 밀가루 8컵이 필요합니다. 그녀는 이미 2컵을 넣었습니다. 그녀는 더 몇 컵을 더 넣어야 할까요?'}]\n",
      "q_response: \n",
      "메리는 이미 2컵의 밀가루를 넣었으므로, 필요한 양인 8컵에서 이미 넣은 2컵을 뺀 나머지를 계산해야 합니다. 8컵 - 2컵 = 6컵 따라서, 메리는 더 6컵의 밀가루를 더 넣어야 합니다.\n",
      "----------------------------\n",
      "prompt : \n",
      "해당 질문에 대한 답변이 맞는지 확인하세요 질문:메리는 케이크를 굽고 있습니다. 레시피에는 밀가루 8컵이 필요합니다. 그녀는 이미 2컵을 넣었습니다. 그녀는 더 몇 컵을 더 넣어야 할까요? 답변:메리는 이미 2컵의 밀가루를 넣었으므로, 필요한 양인 8컵에서 이미 넣은 2컵을 뺀 나머지를 계산해야 합니다. 8컵 - 2컵 = 6컵 따라서, 메리는 더 6컵의 밀가루를 더 넣어야 합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhomes/philhoon/miniconda3/envs/self-debate/lib/python3.8/site-packages/urllib3/connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'aiapi.genielabs.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "  3%|████                                                                                                                                    | 3/102 [00:22<11:33,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : \n",
      "\"메리는 이미 2컵의 밀가루를 넣었으므로\n",
      "----------------------------\n",
      "answer : \n",
      "6.0\n",
      "============================\n",
      "query_msg: \n",
      "[{'role': 'system', 'content': '다음 질문에 차근차근 답하시오.'}, {'role': 'user', 'content': '사라의 고등학교는 올해 12경기의 농구 경기를 했습니다. 그 팀은 대부분의 경기에서 승리했습니다. 그들은 4경기에서 패배했습니다. 그들은 몇 경기를 이겼을까요?'}]\n",
      "q_response: \n",
      "사라의 고등학교는 총 12경기의 농구 경기를 했습니다. 그 중 4경기에서 패배했으므로, 나머지 경기에서 승리한 경기 수를 구해야 합니다. 전체 경기 수에서 패배한 경기 수를 빼면 승리한 경기 수를 구할 수 있습니다. 12경기 - 4경기 = 8경기 따라서, 사라의 고등학교는 8경기를 이겼습니다.\n",
      "----------------------------\n",
      "prompt : \n",
      "해당 질문에 대한 답변이 맞는지 확인하세요 질문:사라의 고등학교는 올해 12경기의 농구 경기를 했습니다. 그 팀은 대부분의 경기에서 승리했습니다. 그들은 4경기에서 패배했습니다. 그들은 몇 경기를 이겼을까요? 답변:사라의 고등학교는 총 12경기의 농구 경기를 했습니다. 그 중 4경기에서 패배했으므로, 나머지 경기에서 승리한 경기 수를 구해야 합니다. 전체 경기 수에서 패배한 경기 수를 빼면 승리한 경기 수를 구할 수 있습니다. 12경기 - 4경기 = 8경기 따라서, 사라의 고등학교는 8경기를 이겼습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhomes/philhoon/miniconda3/envs/self-debate/lib/python3.8/site-packages/urllib3/connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'aiapi.genielabs.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "  4%|█████▎                                                                                                                                  | 4/102 [00:29<11:13,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : \n",
      "\"사라의 고등학교는 4경기를 패배했으므로\n",
      "----------------------------\n",
      "answer : \n",
      "8.0\n",
      "============================\n",
      "query_msg: \n",
      "[{'role': 'system', 'content': '다음 질문에 차근차근 답하시오.'}, {'role': 'user', 'content': '현재 공원에는 호두나무가 22그루 있습니다. 공원 직원들은 오늘 더 많은 호두나무를 심을 예정입니다. 작업이 끝나면 공원에는 총 55그루의 호두나무가 있을 것입니다. 공원 직원들은 오늘 몇 그루의 호두나무를 심었을까요?'}]\n",
      "q_response: \n",
      "공원에 심을 호두나무의 수를 x라고 가정하겠습니다. 현재 공원에 있는 호두나무의 수는 22그루이므로, 작업이 끝난 후에는 22 + x그루의 호두나무가 있을 것입니다. 작업이 끝난 후에는 총 55그루의 호두나무가 있을 것이므로, 다음 식을 세울 수 있습니다. 22 + x = 55 이 식을 풀면, x = 55 - 22 x = 33 따라서, 공원 직원들은 오늘 33그루의 호두나무를 심었습니다.\n",
      "----------------------------\n",
      "prompt : \n",
      "해당 질문에 대한 답변이 맞는지 확인하세요 질문:현재 공원에는 호두나무가 22그루 있습니다. 공원 직원들은 오늘 더 많은 호두나무를 심을 예정입니다. 작업이 끝나면 공원에는 총 55그루의 호두나무가 있을 것입니다. 공원 직원들은 오늘 몇 그루의 호두나무를 심었을까요? 답변:공원에 심을 호두나무의 수를 x라고 가정하겠습니다. 현재 공원에 있는 호두나무의 수는 22그루이므로, 작업이 끝난 후에는 22 + x그루의 호두나무가 있을 것입니다. 작업이 끝난 후에는 총 55그루의 호두나무가 있을 것이므로, 다음 식을 세울 수 있습니다. 22 + x = 55 이 식을 풀면, x = 55 - 22 x = 33 따라서, 공원 직원들은 오늘 33그루의 호두나무를 심었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhomes/philhoon/miniconda3/envs/self-debate/lib/python3.8/site-packages/urllib3/connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'aiapi.genielabs.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "  5%|██████▋                                                                                                                                 | 5/102 [00:38<12:22,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : \n",
      "\"공원 직원들이 오늘 더 많은 호두나무를 심을 예정이므로\n",
      "----------------------------\n",
      "answer : \n",
      "33.0\n",
      "============================\n",
      "query_msg: \n",
      "[{'role': 'system', 'content': '다음 질문에 차근차근 답하시오.'}, {'role': 'user', 'content': '앨리사는 용돈의 절반을 영화 보는 데 사용했습니다. 그녀는 가족 차량을 세척하여 8달러를 벌었습니다. 그렇다면 그녀의 주간 용돈은 얼마인가요?'}]\n",
      "q_response: \n",
      "앨리사가 영화를 보는 데 사용한 돈의 절반은 8달러입니다. 따라서 영화를 보는 데 사용한 돈은 8달러 * 2 = 16달러입니다. 앨리사의 주간 용돈은 영화를 보는 데 사용한 돈의 2배인 16달러입니다.\n",
      "----------------------------\n",
      "prompt : \n",
      "해당 질문에 대한 답변이 맞는지 확인하세요 질문:앨리사는 용돈의 절반을 영화 보는 데 사용했습니다. 그녀는 가족 차량을 세척하여 8달러를 벌었습니다. 그렇다면 그녀의 주간 용돈은 얼마인가요? 답변:앨리사가 영화를 보는 데 사용한 돈의 절반은 8달러입니다. 따라서 영화를 보는 데 사용한 돈은 8달러 * 2 = 16달러입니다. 앨리사의 주간 용돈은 영화를 보는 데 사용한 돈의 2배인 16달러입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhomes/philhoon/miniconda3/envs/self-debate/lib/python3.8/site-packages/urllib3/connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'aiapi.genielabs.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "  5%|██████▋                                                                                                                                 | 5/102 [00:44<14:16,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : \n",
      "\"앨리사의 주간 용돈은 8*2=16달러입니다. 그러므로\n",
      "----------------------------\n",
      "answer : \n",
      "8.0\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for ins in tqdm(data):\n",
    "    if 'kor_question' in ins:\n",
    "        kor_que = ins['kor_question']\n",
    "    else:\n",
    "        kor_que = ins['question']\n",
    "        \n",
    "    if 'answer_s' in ins:\n",
    "        answer = ins['answer_s']\n",
    "    else:\n",
    "        answer = ins['answer']\n",
    "    \n",
    "    query = remove_line_split(kor_que)\n",
    "    \n",
    "    query_msg = get_kor_msg(query)\n",
    "\n",
    "    query_completion = generate_answer(query_msg, model_name, max_token, temperature)\n",
    "    q_response = query_completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    \n",
    "    q_response = remove_line_split(q_response)\n",
    "    \n",
    "    print(f\"query_msg: \\n{query_msg}\")\n",
    "    print(f\"q_response: \\n{q_response}\")\n",
    "    \n",
    "    print('----------------------------')\n",
    "    prompt = get_kt_verfiy(kor_que, q_response)\n",
    "    print(f\"prompt : \\n{prompt}\")\n",
    "    body = get_body(prompt, BODY_TEMP)\n",
    "    response = model_kt.generate(body)\n",
    "    res_context = response.json()['data'][0]['result'][0]\n",
    "    print(f\"response : \\n{res_context}\")\n",
    "    print(f\"----------------------------\")\n",
    "    print(f\"answer : \\n{answer}\")\n",
    "    print(f\"============================\")\n",
    "    \n",
    "\n",
    "#     ans_completion = generate_answer(ans_msg, model_name, max_token, temperature)\n",
    "#     ans_response = ans_completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "#     instance['kor_query'] = q_response\n",
    "#     instance['kor_ans'] = ans_response\n",
    "#     pprint(instance)\n",
    "#     jsonl_writer.write_json_line(ins)a\n",
    "    if cnt == 5:\n",
    "        break\n",
    "    else:\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a92cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab44e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# body = get_body('hi', BODY_TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da11716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = model_kt.generate(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a5a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01837061",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(api_kt['url'], data=body, headers=api_kt['headers'], verify=False)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cf2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-debate",
   "language": "python",
   "name": "self-debate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
