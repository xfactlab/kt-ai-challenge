{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4b4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import yaml\n",
    "from src import utils\n",
    "import time\n",
    "from pprint import pprint\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "341f69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT = {\n",
    "#     '3p' : get_3p_prompt,\n",
    "#     'cot' : get_CoT_prompt,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "784da61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = 'openai'\n",
    "api_key_file = '/userhomes/philhoon/kt-ai-challenge/api-openai.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da6c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name=\"gpt-3.5-turbo-0301\",\n",
    "model_name = \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "max_token=2048\n",
    "temperature=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd66a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/userhomes/philhoon/kt-ai-challenge/data/public_mwp_data_v2_preprocess.jsonl'\n",
    "# output_path = '/userhomes/philhoon/self-debate/result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai key\n",
    "key_dict = utils.get_keys(api_key_file)\n",
    "openai.api_key = key_dict[api]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e421b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '/userhomes/philhoon/kt-ai-challenge/data/public_mwp_data_v2_reasoning.jsonl'\n",
    "print(f'output_file : {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7283f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai-gpt-3.5-turbo-0301-2048-0.0-gsm8k-test.jsonl -> openai-gpt-3.5-turbo-0301-3p-2048-0.0-gsm8k-test.jsonl\n",
    "# openai-gpt-3.5-turbo-0301-gsm8k-test.jsonl -> openai-gpt-3.5-turbo-0301-3p-inf-1.0-gsm8k-test.jsonl\n",
    "# openai-gpt-3.5-turbo-0301-2048-0.0-gsm8k-test.jsonl -> openai-gpt-3.5-turbo-0301-3p-2048-0.0-gsm8k-test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2203391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file = output_path + '/' + 'gsm8k-korean.jsonl'\n",
    "# print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jsonlines writier\n",
    "jsonl_writer = utils.JSONLWriter(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b79621",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.read_jsonlines(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224500f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for ins in data:\n",
    "# #     pprint(ins)\n",
    "#     print(ins['question'])\n",
    "#     print(ins['answer'])\n",
    "#     print('----')\n",
    "#     cnt += 1\n",
    "#     if cnt == 20:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_message(prompt):\n",
    "#     \"\"\"\n",
    "#     Message for chat api\n",
    "#     \"\"\"\n",
    "#     message = {\n",
    "#         \"role\" : \"user\",\n",
    "#         \"content\" : prompt\n",
    "#         }\n",
    "#     return [message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(message, model_name, max_token, temperature):\n",
    "    \"\"\"\n",
    "    Generate response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=message,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_token,\n",
    "            n=1)\n",
    "    except:\n",
    "        print(\"retrying due to an error......\")\n",
    "        time.sleep(20)\n",
    "        return generate_answer(message, model_name, max_token, temperature)\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_trans_msg(context):\n",
    "#     \"\"\"\n",
    "#     Message for chat api\n",
    "#     \"\"\"\n",
    "\n",
    "#     msg = [\n",
    "#         {\n",
    "#             \"role\" : \"system\",\n",
    "#             \"content\" : \"Translate given setences into Korean.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\" : \"user\",\n",
    "#             \"content\" : context\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "#     return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reason_mgs(question, answer):\n",
    "    \"\"\"\n",
    "    Message for chat api\n",
    "    주어진 질문과 답에 대해, 풀이를 수식으로 300자이내로 서술하시오.\n",
    "    질문: 지름이 12cm, 높이가 14cm인 원기둥의 전개도를 그렸을 때 전개도의 옆면의 둘레는 몇 cm인지 답을 구하시오. (원주율: 3) 답: 100\n",
    "    \"\"\"\n",
    "\n",
    "    msg = [\n",
    "        {\n",
    "            \"role\" : \"system\",\n",
    "            \"content\" : \"주어진 질문과 정답에 대해, 풀이를 수식으로 300자이내로 서술하시오.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : f\"질문: {question} 정답:{answer}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf28b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790edff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b206f84e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for instance in tqdm(data):\n",
    "    query = instance['question']\n",
    "    ans = instance['answer']\n",
    "        \n",
    "    query_msg = get_reason_mgs(query, ans)\n",
    "    \n",
    "    \n",
    "    query_completion = generate_answer(query_msg, model_name, max_token, temperature)\n",
    "    q_response = query_completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "#     print('query_mgs')\n",
    "#     print(query_msg)\n",
    "#     print('q_response')\n",
    "#     print(q_response)\n",
    "    \n",
    "    instance['reason'] = q_response\n",
    "    print(query_msg)\n",
    "    pprint(instance)\n",
    "\n",
    "#     jsonl_writer.write_json_line(instance)\n",
    "    cnt += 1\n",
    "    if cnt == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67b30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d84ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0686d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2240a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for instance in data:\n",
    "#     question = instance['question']\n",
    "#     answer_s = utils.extract_answer(instance['answer'])\n",
    "#     prompt = get_CoT_prompt(question)\n",
    "#     prompt = get_prompt(question)\n",
    "#     print(f'prompt : \\n {prompt}')\n",
    "#     message = get_message(prompt)\n",
    "    \n",
    "#     continue\n",
    "#     completion = generate_answer(message, model_name)\n",
    "    \n",
    "# #     completion = openai.ChatCompletion.create(\n",
    "# #                   model=model_name,\n",
    "# #                   messages=message,\n",
    "# #                   temperature=0,Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\n",
    "# #                   max_tokens=4096,\n",
    "# #                   n=1)\n",
    "    \n",
    "    \n",
    "#     response = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "# #     instance['response'] = response\n",
    "# #     instance['answer_specific'] = answer_s\n",
    "# #     jsonl_writer.write_json_line(instance)\n",
    "#     print(f'respones : \\n {response}')\n",
    "#     cnt += 1\n",
    "#     if cnt == 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcec2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4af3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-debate",
   "language": "python",
   "name": "self-debate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
