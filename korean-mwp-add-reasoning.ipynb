{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import yaml\n",
    "from src import utils\n",
    "import time\n",
    "from pprint import pprint\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from src.prompts import get_CoT_prompt, get_3p_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT = {\n",
    "#     '3p' : get_3p_prompt,\n",
    "#     'cot' : get_CoT_prompt,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784da61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = 'openai'\n",
    "api_key_file = '/userhomes/philhoon/self-debate/api.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da6c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name=\"gpt-3.5-turbo-0301\",\n",
    "model_name = \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "max_token=2048\n",
    "temperature=0.0\n",
    "prompt_type='cot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd66a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name='gsm8k'\n",
    "input_file = '/projects/self-debate/data/gsm8k/test.jsonl'\n",
    "output_path = '/userhomes/philhoon/self-debate/result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai key\n",
    "key_dict = utils.get_keys(api_key_file)\n",
    "openai.api_key = key_dict[api]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e421b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type = input_file.split('/')[-1].split('.')[-2]\n",
    "output_file_name = f'{api}-{model_name}-{max_token}-{temperature}-KOR-{data_name}-{file_type}.jsonl'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "output_file = output_path + '/' + output_file_name\n",
    "print(f'output_file : {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7283f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai-gpt-3.5-turbo-0301-2048-0.0-gsm8k-test.jsonl -> openai-gpt-3.5-turbo-0301-3p-2048-0.0-gsm8k-test.jsonl\n",
    "# openai-gpt-3.5-turbo-0301-gsm8k-test.jsonl -> openai-gpt-3.5-turbo-0301-3p-inf-1.0-gsm8k-test.jsonl\n",
    "# openai-gpt-3.5-turbo-0301-2048-0.0-gsm8k-test.jsonl -> openai-gpt-3.5-turbo-0301-3p-2048-0.0-gsm8k-test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2203391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file = output_path + '/' + 'gsm8k-korean.jsonl'\n",
    "# print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jsonlines writier\n",
    "jsonl_writer = utils.JSONLWriter(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b79621",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.read_json(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message(prompt):\n",
    "    \"\"\"\n",
    "    Message for chat api\n",
    "    \"\"\"\n",
    "    message = {\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : prompt\n",
    "        }\n",
    "    return [message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(message, model_name, max_token, temperature):\n",
    "    \"\"\"\n",
    "    Generate response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=message,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_token,\n",
    "            n=1)\n",
    "    except:\n",
    "        print(\"retrying due to an error......\")\n",
    "        time.sleep(20)\n",
    "        return generate_answer(message, model_name, max_token, temperature)\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trans_msg(context):\n",
    "    \"\"\"\n",
    "    Message for chat api\n",
    "    \"\"\"\n",
    "\n",
    "    msg = [\n",
    "        {\n",
    "            \"role\" : \"system\",\n",
    "            \"content\" : \"Translate given setences into Korean.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : context\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf28b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790edff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b206f84e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "for instance in tqdm(data):\n",
    "    query = instance['question']\n",
    "    ans = instance['answer']\n",
    "    answer_s = utils.extract_answer(instance['answer'])\n",
    "        \n",
    "    query_msg = get_trans_msg(query)\n",
    "    ans_msg = get_trans_msg(ans)\n",
    "    \n",
    "    \n",
    "    query_completion = generate_answer(query_msg, model_name, max_token, temperature)\n",
    "    q_response = query_completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    ans_completion = generate_answer(ans_msg, model_name, max_token, temperature)\n",
    "    ans_response = ans_completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    instance['kor_question'] = q_response\n",
    "    instance['kor_answer'] = ans_response\n",
    "    instance['answer_s'] = answer_s\n",
    "    \n",
    "    pprint(instance)\n",
    "#     jsonl_writer.write_json_line(instance)\n",
    "#     cnt += 1\n",
    "#     if cnt == 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67b30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d84ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0686d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2240a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for instance in data:\n",
    "#     question = instance['question']\n",
    "#     answer_s = utils.extract_answer(instance['answer'])\n",
    "#     prompt = get_CoT_prompt(question)\n",
    "#     prompt = get_prompt(question)\n",
    "#     print(f'prompt : \\n {prompt}')\n",
    "#     message = get_message(prompt)\n",
    "    \n",
    "#     continue\n",
    "#     completion = generate_answer(message, model_name)\n",
    "    \n",
    "# #     completion = openai.ChatCompletion.create(\n",
    "# #                   model=model_name,\n",
    "# #                   messages=message,\n",
    "# #                   temperature=0,Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\n",
    "# #                   max_tokens=4096,\n",
    "# #                   n=1)\n",
    "    \n",
    "    \n",
    "#     response = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "# #     instance['response'] = response\n",
    "# #     instance['answer_specific'] = answer_s\n",
    "# #     jsonl_writer.write_json_line(instance)\n",
    "#     print(f'respones : \\n {response}')\n",
    "#     cnt += 1\n",
    "#     if cnt == 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcec2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4af3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-debate",
   "language": "python",
   "name": "self-debate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
